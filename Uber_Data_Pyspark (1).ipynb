{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b84d8b-6aa8-4675-a024-95f4ee764ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503ada43-784d-4178-b0b0-f879798d7543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in ./pyspark_env/lib/python3.13/site-packages (4.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in ./pyspark_env/lib/python3.13/site-packages (from pyspark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f324b65-e9ae-4ba6-bb64-822352d8a00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/11 10:06:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TestSpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dfcbb20-39d9-42c0-83d5-ae93eb130ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Users/DELL/Downloads/dataset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5270d2fa-4bde-4bd0-97c8-e762a4cde693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='10-Sep-12', Time (Local)=7, Eyeballs =5, Zeroes =0, Completed Trips =2, Requests =2, Unique Drivers=9),\n",
       " Row(Date=None, Time (Local)=8, Eyeballs =6, Zeroes =0, Completed Trips =2, Requests =2, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=9, Eyeballs =8, Zeroes =3, Completed Trips =0, Requests =0, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=10, Eyeballs =9, Zeroes =2, Completed Trips =0, Requests =1, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=11, Eyeballs =11, Zeroes =1, Completed Trips =4, Requests =4, Unique Drivers=11)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9708e35d-3fa3-4b70-a1a7-ecf6b60a5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using spark api\n",
    "completed_trips_by_date = df.groupBy(\"Date\").sum(\"Completed Trips \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c08fca6-235c-4e38-9864-cda97f018d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, sum(Completed Trips ): bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_trips_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf3d6e47-6614-44a4-8209-6afd5923c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Date with most completed trips: None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, desc\n",
    "\n",
    "# Group by and compute total\n",
    "completed_trips_by_date = df.groupBy(\"Date\").agg(_sum(\"Completed Trips\").alias(\"TotalTrips\"))\n",
    "\n",
    "# Find date with most completed trips\n",
    "date_with_most_completed_trips = completed_trips_by_date \\\n",
    "    .orderBy(desc(\"TotalTrips\")) \\\n",
    "    .select(\"Date\") \\\n",
    "    .first()[\"Date\"]\n",
    "\n",
    "print(\"üìÖ Date with most completed trips:\", date_with_most_completed_trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5abf2811-c833-4baa-a47d-dc7f2819ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     Date|TotalTrips|\n",
      "+---------+----------+\n",
      "|15-Sep-12|        23|\n",
      "|23-Sep-12|        23|\n",
      "|22-Sep-12|        19|\n",
      "|16-Sep-12|        17|\n",
      "|21-Sep-12|        10|\n",
      "|18-Sep-12|         3|\n",
      "|14-Sep-12|         3|\n",
      "|20-Sep-12|         2|\n",
      "|10-Sep-12|         2|\n",
      "|11-Sep-12|         1|\n",
      "|24-Sep-12|         1|\n",
      "|13-Sep-12|         0|\n",
      "|17-Sep-12|         0|\n",
      "|12-Sep-12|         0|\n",
      "|19-Sep-12|         0|\n",
      "+---------+----------+\n",
      "\n",
      "üìÖ Date with most completed trips: 15-Sep-12 üöó Trips: 23\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, desc\n",
    "\n",
    "# Strip spaces from column names\n",
    "uber = uber.select([col(c).alias(c.strip()) for c in uber.columns])\n",
    "\n",
    "# Drop rows where Date is null\n",
    "uber = uber.filter(col(\"Date\").isNotNull())\n",
    "\n",
    "# Ensure 'Completed Trips' is integer\n",
    "uber = uber.withColumn(\"Completed Trips\", col(\"Completed Trips\").cast(\"int\"))\n",
    "\n",
    "# Group and find total trips\n",
    "completed_trips_by_date = uber.groupBy(\"Date\").agg(_sum(\"Completed Trips\").alias(\"TotalTrips\"))\n",
    "\n",
    "# Order by total trips\n",
    "completed_trips_by_date.orderBy(desc(\"TotalTrips\")).show()\n",
    "\n",
    "# Find date with most trips\n",
    "top_row = completed_trips_by_date.orderBy(desc(\"TotalTrips\")).first()\n",
    "if top_row:\n",
    "    print(\"üìÖ Date with most completed trips:\", top_row[\"Date\"], \"üöó Trips:\", top_row[\"TotalTrips\"])\n",
    "else:\n",
    "    print(\"‚ùå No valid data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5425de90-1b2f-4c82-97c5-19e77700399f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='10-Sep-12', Time (Local)=7, Eyeballs=5, Zeroes=0, Completed Trips=2, Requests=2, Unique Drivers=9),\n",
       " Row(Date='11-Sep-12', Time (Local)=0, Eyeballs=9, Zeroes=3, Completed Trips=1, Requests=1, Unique Drivers=3),\n",
       " Row(Date='12-Sep-12', Time (Local)=0, Eyeballs=9, Zeroes=2, Completed Trips=0, Requests=1, Unique Drivers=2),\n",
       " Row(Date='13-Sep-12', Time (Local)=0, Eyeballs=11, Zeroes=11, Completed Trips=0, Requests=2, Unique Drivers=0),\n",
       " Row(Date='14-Sep-12', Time (Local)=0, Eyeballs=10, Zeroes=1, Completed Trips=3, Requests=4, Unique Drivers=3)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber.head(5) #uber has the grouped date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e4e8d1d-527d-44de-9f96-5a58da9f8564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='10-Sep-12', Time (Local)=7, Eyeballs=5, Zeroes=0, Completed Trips=2, Requests=2, Unique Drivers=9),\n",
       " Row(Date=None, Time (Local)=8, Eyeballs=6, Zeroes=0, Completed Trips=2, Requests=2, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=9, Eyeballs=8, Zeroes=3, Completed Trips=0, Requests=0, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=10, Eyeballs=9, Zeroes=2, Completed Trips=0, Requests=1, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=11, Eyeballs=11, Zeroes=1, Completed Trips=4, Requests=4, Unique Drivers=11)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e63935d9-9b88-41c1-98d1-35bbe24e2256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 10:32:30 ERROR Executor: Exception in task 0.0 in stage 44.0 (TID 35)\n",
      "org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '08:00' could not be parsed at index 2. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/11 10:32:30 WARN TaskSetManager: Lost task 0.0 in stage 44.0 (TID 35) (macbookair.mshome.net executor driver): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '08:00' could not be parsed at index 2. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/06/11 10:32:30 ERROR TaskSetManager: Task 0 in stage 44.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "DateTimeException",
     "evalue": "[CANNOT_PARSE_TIMESTAMP] Text '08:00' could not be parsed at index 2. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDateTimeException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     14\u001b[39m completed_trips_by_window = df \\\n\u001b[32m     15\u001b[39m     .groupBy(window(\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m24 hours\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     16\u001b[39m     .agg(\u001b[38;5;28msum\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCompleted Trips\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mTotal Completed Trips\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     17\u001b[39m     .orderBy(\u001b[33m\"\u001b[39m\u001b[33mTotal Completed Trips\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Step 4: Get the highest number of completed trips in a 24-hour window\u001b[39;00m\n\u001b[32m     20\u001b[39m highest_completed_trips_in_24_hours = \u001b[43mcompleted_trips_by_window\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTotal Completed Trips\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mTotal Completed Trips\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöó Highest completed trips in any 24-hour window:\u001b[39m\u001b[33m\"\u001b[39m, highest_completed_trips_in_24_hours)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:940\u001b[39m, in \u001b[36mDataFrame.first\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Optional[Row]:\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:935\u001b[39m, in \u001b[36mDataFrame.head\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    933\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhead\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Union[Optional[Row], List[Row]]:\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m935\u001b[39m         rs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    937\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.take(n)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:937\u001b[39m, in \u001b[36mDataFrame.head\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    935\u001b[39m     rs = \u001b[38;5;28mself\u001b[39m.head(\u001b[32m1\u001b[39m)\n\u001b[32m    936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:460\u001b[39m, in \u001b[36mDataFrame.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) -> List[Row]:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:443\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Row]:\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m         sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mDateTimeException\u001b[39m: [CANNOT_PARSE_TIMESTAMP] Text '08:00' could not be parsed at index 2. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, lpad, to_timestamp, sum, window, lit\n",
    "\n",
    "\n",
    "# Step 1: Combine Date and Time into a proper datetime string\n",
    "df = df.withColumn(\n",
    "    \"datetime_string\",\n",
    "    concat_ws(\" \", \"Date\", concat_ws(\":\", lpad(\"Time (Local)\", 2, \"0\"), lpad(lit(\"00\"), 2, \"0\")))\n",
    ")\n",
    "\n",
    "# Step 2: Convert to timestamp (handle format like '15-Sep-12 16:00')\n",
    "df = df.withColumn(\"datetime\", to_timestamp(\"datetime_string\", \"dd-MMM-yy HH:mm\"))\n",
    "\n",
    "# Step 3: Group by 24-hour window and sum the completed trips\n",
    "completed_trips_by_window = df \\\n",
    "    .groupBy(window(\"datetime\", \"24 hours\")) \\\n",
    "    .agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\")) \\\n",
    "    .orderBy(\"Total Completed Trips\", ascending=False)\n",
    "\n",
    "# Step 4: Get the highest number of completed trips in a 24-hour window\n",
    "highest_completed_trips_in_24_hours = completed_trips_by_window \\\n",
    "    .select(\"Total Completed Trips\") \\\n",
    "    .first()[\"Total Completed Trips\"]\n",
    "\n",
    "print(\"üöó Highest completed trips in any 24-hour window:\", highest_completed_trips_in_24_hours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d97f2-f2b6-4c06-a114-2d447e716b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a47b9-4a9b-4b38-a518-ee06efe3cd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "047df01b-ea68-4789-b770-9413e553b604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-06-11 10:33:16.128\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \\\"hour(Time (Local))\\\" due to data type mismatch: The first parameter requires the \\\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\\\" type, however \\\"Time (Local)\\\" has the type \\\"INT\\\". SQLSTATE: 42K09\", \"context\": {\"file\": \"java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"line\": \"\", \"fragment\": \"hour\", \"errorClass\": \"DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o695.agg.\\n: org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \\\"hour(Time (Local))\\\" due to data type mismatch: The first parameter requires the \\\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\\\" type, however \\\"Time (Local)\\\" has the type \\\"INT\\\". SQLSTATE: 42K09;\\n'Aggregate [hour(Time (Local)#42, Some(Asia/Kolkata))], [hour(Time (Local)#42, Some(Asia/Kolkata)) AS hour#428, sum(Requests#46) AS total_requests#429L]\\n+- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#407, to_timestamp(datetime_string#407, Some(dd-MMM-yy HH:mm), TimestampType, Some(Asia/Kolkata), true) AS datetime#408]\\n   +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, concat_ws(:, lpad(cast(Time (Local)#42 as string), 2, 0), lpad(00, 2, 0))) AS datetime_string#407, datetime#387]\\n      +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#386, to_timestamp(datetime_string#386, Some(dd-MMM-yy HH), TimestampType, Some(Asia/Kolkata), true) AS datetime#387]\\n         +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, lpad(cast(Time (Local)#42 as string), 2, 0)) AS datetime_string#386]\\n            +- Project [Date#17 AS Date#41, Time (Local)#18 AS Time (Local)#42, Eyeballs #19 AS Eyeballs#43, Zeroes #20 AS Zeroes#44, Completed Trips #21 AS Completed Trips#45, Requests #22 AS Requests#46, Unique Drivers#23 AS Unique Drivers#47]\\n               +- Relation [Date#17,Time (Local)#18,Eyeballs #19,Zeroes #20,Completed Trips #21,Requests #22,Unique Drivers#23] csv\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:77)\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:70)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:420)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:84)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:54)\\n\\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:158)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:147)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:77)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:70)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:420)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 22 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/Users/DELL/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/Users/DELL/Downloads/pyspark_env/lib/python3.13/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"hour(Time (Local))\" due to data type mismatch: The first parameter requires the \"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\" type, however \"Time (Local)\" has the type \"INT\". SQLSTATE: 42K09;\n'Aggregate [hour(Time (Local)#42, Some(Asia/Kolkata))], [hour(Time (Local)#42, Some(Asia/Kolkata)) AS hour#428, sum(Requests#46) AS total_requests#429L]\n+- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#407, to_timestamp(datetime_string#407, Some(dd-MMM-yy HH:mm), TimestampType, Some(Asia/Kolkata), true) AS datetime#408]\n   +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, concat_ws(:, lpad(cast(Time (Local)#42 as string), 2, 0), lpad(00, 2, 0))) AS datetime_string#407, datetime#387]\n      +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#386, to_timestamp(datetime_string#386, Some(dd-MMM-yy HH), TimestampType, Some(Asia/Kolkata), true) AS datetime#387]\n         +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, lpad(cast(Time (Local)#42 as string), 2, 0)) AS datetime_string#386]\n            +- Project [Date#17 AS Date#41, Time (Local)#18 AS Time (Local)#42, Eyeballs #19 AS Eyeballs#43, Zeroes #20 AS Zeroes#44, Completed Trips #21 AS Completed Trips#45, Requests #22 AS Requests#46, Unique Drivers#23 AS Unique Drivers#47]\n               +- Relation [Date#17,Time (Local)#18,Eyeballs #19,Zeroes #20,Completed Trips #21,Requests #22,Unique Drivers#23] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hour, \u001b[38;5;28msum\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m hourly_requests = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhour\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTime (Local)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhour\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRequests\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal_requests\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.orderBy(\u001b[33m\"\u001b[39m\u001b[33mtotal_requests\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      5\u001b[39m most_requested_hour = hourly_requests.select(\u001b[33m\"\u001b[39m\u001b[33mhour\u001b[39m\u001b[33m\"\u001b[39m).first()[\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThe hour with the most requests is:\u001b[39m\u001b[33m\"\u001b[39m, most_requested_hour)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/group.py:190\u001b[39m, in \u001b[36mGroupedData.agg\u001b[39m\u001b[34m(self, *exprs)\u001b[39m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[33m\"\u001b[39m\u001b[33mall exprs should be Column\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m     exprs = cast(Tuple[Column, ...], exprs)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jgd\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.session)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"hour(Time (Local))\" due to data type mismatch: The first parameter requires the \"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\" type, however \"Time (Local)\" has the type \"INT\". SQLSTATE: 42K09;\n'Aggregate [hour(Time (Local)#42, Some(Asia/Kolkata))], [hour(Time (Local)#42, Some(Asia/Kolkata)) AS hour#428, sum(Requests#46) AS total_requests#429L]\n+- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#407, to_timestamp(datetime_string#407, Some(dd-MMM-yy HH:mm), TimestampType, Some(Asia/Kolkata), true) AS datetime#408]\n   +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, concat_ws(:, lpad(cast(Time (Local)#42 as string), 2, 0), lpad(00, 2, 0))) AS datetime_string#407, datetime#387]\n      +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#386, to_timestamp(datetime_string#386, Some(dd-MMM-yy HH), TimestampType, Some(Asia/Kolkata), true) AS datetime#387]\n         +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, lpad(cast(Time (Local)#42 as string), 2, 0)) AS datetime_string#386]\n            +- Project [Date#17 AS Date#41, Time (Local)#18 AS Time (Local)#42, Eyeballs #19 AS Eyeballs#43, Zeroes #20 AS Zeroes#44, Completed Trips #21 AS Completed Trips#45, Requests #22 AS Requests#46, Unique Drivers#23 AS Unique Drivers#47]\n               +- Relation [Date#17,Time (Local)#18,Eyeballs #19,Zeroes #20,Completed Trips #21,Requests #22,Unique Drivers#23] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, sum\n",
    "\n",
    "hourly_requests = df.groupBy(hour(\"Time (Local)\").alias(\"hour\")).agg(sum(\"Requests\").alias(\"total_requests\")).orderBy(\"total_requests\", ascending=False)\n",
    "\n",
    "most_requested_hour = hourly_requests.select(\"hour\").first()[0]\n",
    "print(\"The hour with the most requests is:\", most_requested_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dda7ecb5-4459-4b2e-86ed-215714b0a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|Time (Local)|TotalRequests|\n",
      "+------------+-------------+\n",
      "|          23|          184|\n",
      "+------------+-------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#3. Which hour of the day had the most requests during the two-week period?\n",
    "df.groupBy(\"Time (Local)\") \\\n",
    "    .agg(sum(\"Requests\").alias(\"TotalRequests\")) \\\n",
    "    .orderBy(desc(\"TotalRequests\")) \\\n",
    "    .show(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "153d7a03-bbe6-4f0f-958d-ffdcaaa6abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Weekend Zeroes: 2.31%\n"
     ]
    }
   ],
   "source": [
    "#4What percentage of all zeroes occurred on the weekend (Fri 5pm to Sun 3am)?\n",
    "from pyspark.sql.functions import hour, to_timestamp, date_format\n",
    "\n",
    "df = df.withColumn(\"Timestamp\", to_timestamp(\"Date\", \"dd-MMM-yy\")) \\\n",
    "       .withColumn(\"Weekday\", date_format(\"Timestamp\", \"E\")) \\\n",
    "       .withColumn(\"Hour\", df[\"Time (Local)\"].cast(\"int\"))\n",
    "\n",
    "# Weekend filter: Fri 17:00 - Sun 03:00\n",
    "weekend_df = df.filter(\n",
    "    ((df[\"Weekday\"] == \"Fri\") & (df[\"Hour\"] >= 17)) |\n",
    "    (df[\"Weekday\"] == \"Sat\") |\n",
    "    ((df[\"Weekday\"] == \"Sun\") & (df[\"Hour\"] < 3))\n",
    ")\n",
    "\n",
    "# Sum Zeroes\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "total_zeroes = df.agg(sum(\"Zeroes\")).collect()[0][0]\n",
    "weekend_zeroes = weekend_df.agg(sum(\"Zeroes\")).collect()[0][0]\n",
    "\n",
    "percentage = (weekend_zeroes / total_zeroes) * 100\n",
    "print(f\"üìä Weekend Zeroes: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18a311aa-7d11-4306-ab41-7e8468515d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñ Weighted average: 0.83 trips per driver\n"
     ]
    }
   ],
   "source": [
    "#Q5.What is the weighted average ratio of completed trips per driver during the two-week period? Tip: ‚ÄúWeighted average‚Äù means your answer should account for the total trip volume in each hour to determine the most accurate number in the whole period.\n",
    "from pyspark.sql.functions import col, expr, sum\n",
    "\n",
    "# Safe division: try_divide avoids division-by-zero error\n",
    "df = df.withColumn(\"TripDriverRatio\", expr(\"try_divide(`Completed Trips`, `Unique Drivers`)\"))\n",
    "\n",
    "# Weighted average = sum(ratio * completed trips) / sum(completed trips)\n",
    "df = df.withColumn(\"Weight\", col(\"TripDriverRatio\") * col(\"Completed Trips\"))\n",
    "\n",
    "weighted_avg = df.agg((sum(\"Weight\") / sum(\"Completed Trips\")).alias(\"WeightedAvg\")) \\\n",
    "                 .first()[\"WeightedAvg\"]\n",
    "\n",
    "print(f\"‚öñ Weighted average: {weighted_avg:.2f} trips per driver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcfbb8ee-84b9-4416-af2d-6d15c18c2a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|  Shift|TotalRequests|\n",
      "+-------+-------------+\n",
      "|Shift_3|         1032|\n",
      "|Shift_1|          446|\n",
      "|Shift_2|          380|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q6.In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two-week period in terms of unique requests? A new shift starts every 8 hours. Assume that a driver will work the same shift each day.\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\"Shift\", when(col(\"Time (Local)\") < 8, \"Shift_1\")\n",
    "                              .when(col(\"Time (Local)\") < 16, \"Shift_2\")\n",
    "                              .otherwise(\"Shift_3\"))\n",
    "\n",
    "df_shift = df.groupBy(\"Shift\").agg(sum(\"Requests\").alias(\"TotalRequests\")) \\\n",
    "             .orderBy(desc(\"TotalRequests\"))\n",
    "\n",
    "df_shift.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59c795f7-dfd2-4721-8699-bd372ba2a753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Correlation: 0.69\n",
      "‚úÖ True\n"
     ]
    }
   ],
   "source": [
    "#Q7.True or False: Driver supply always increases when demand increases during the two-week period. Tip: Visualize the data to confirm your answer if needed.\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "correlation = df.select(corr(\"Requests\", \"Unique Drivers\")).first()[0]\n",
    "print(f\"üìà Correlation: {correlation:.2f}\")\n",
    "print(\"‚úÖ True\" if correlation > 0.5 else \"‚ùå False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92da7dd2-9daf-432e-8910-d26d9acb8f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 11:07:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/11 11:07:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/11 11:07:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/11 11:07:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/11 11:07:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|     Date|            Ratio|\n",
      "+---------+-----------------+\n",
      "|23-Sep-12|0.391304347826087|\n",
      "+---------+-----------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#Q8.In which 72-hour period is the ratio of Zeroes to Eyeballs the highest?\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Window over 3 days\n",
    "df = df.withColumn(\"EyeballsInt\", col(\"Eyeballs\").cast(\"int\")) \\\n",
    "       .withColumn(\"ZeroesInt\", col(\"Zeroes\").cast(\"int\"))\n",
    "\n",
    "windowSpec = Window.orderBy(\"Date\").rowsBetween(0, 71)\n",
    "\n",
    "df = df.withColumn(\"SumZeroes\", sum(\"ZeroesInt\").over(windowSpec)) \\\n",
    "       .withColumn(\"SumEyeballs\", sum(\"EyeballsInt\").over(windowSpec)) \\\n",
    "       .withColumn(\"Ratio\", col(\"SumZeroes\") / col(\"SumEyeballs\")) \\\n",
    "       .orderBy(desc(\"Ratio\"))\n",
    "\n",
    "df.select(\"Date\", \"Ratio\").show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78d986e0-fba4-4a9e-9b79-96e9cb0a7a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+------------+-----------------+\n",
      "|Time (Local)|TotalEyeballs|TotalDrivers| EyeballPerDriver|\n",
      "+------------+-------------+------------+-----------------+\n",
      "|          23|          488|         119|4.100840336134453|\n",
      "+------------+-------------+------------+-----------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#9. If you could add 5 drivers to a single hour daily, which one?\n",
    "df.groupBy(\"Time (Local)\") \\\n",
    "    .agg(\n",
    "        sum(\"Eyeballs\").alias(\"TotalEyeballs\"),\n",
    "        sum(\"Unique Drivers\").alias(\"TotalDrivers\")\n",
    "    ) \\\n",
    "    .withColumn(\"EyeballPerDriver\", col(\"TotalEyeballs\") / col(\"TotalDrivers\")) \\\n",
    "    .orderBy(desc(\"EyeballPerDriver\")) \\\n",
    "    .show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a20bd721-d94e-4533-a8b0-5307fdce52b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Best hour to add drivers: 23h\n",
      "üßÆ Pressure score: 4.10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col, desc\n",
    "\n",
    "# Group by hour of day (0-23) and calculate total eyeballs and total drivers\n",
    "hourly_demand = df.groupBy(\"Time (Local)\").agg(\n",
    "    sum(\"Eyeballs\").alias(\"TotalEyeballs\"),\n",
    "    sum(\"Unique Drivers\").alias(\"TotalDrivers\")\n",
    ")\n",
    "\n",
    "# Calculate pressure score = Eyeballs per driver, safely avoiding division by zero\n",
    "hourly_demand = hourly_demand.withColumn(\n",
    "    \"PressureScore\",\n",
    "    expr(\"try_divide(TotalEyeballs, TotalDrivers)\")\n",
    ")\n",
    "\n",
    "# Find the hour with the highest pressure score\n",
    "busiest_hour = hourly_demand.orderBy(desc(\"PressureScore\")).first()\n",
    "\n",
    "print(f\"‚è∞ Best hour to add drivers: {busiest_hour['Time (Local)']}h\")\n",
    "print(f\"üßÆ Pressure score: {busiest_hour['PressureScore']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec0a40f5-1ed0-4d3b-a158-06ea24455464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------+------------+-------------+\n",
      "|Time (Local)|TotalEyeballs|TotalRequests|TotalDrivers|TotalActivity|\n",
      "+------------+-------------+-------------+------------+-------------+\n",
      "|           4|           29|            9|           9|           47|\n",
      "+------------+-------------+-------------+------------+-------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#Q10.\n",
    "df.groupBy(\"Time (Local)\") \\\n",
    "    .agg(\n",
    "        sum(\"Eyeballs\").alias(\"TotalEyeballs\"),\n",
    "        sum(\"Requests\").alias(\"TotalRequests\"),\n",
    "        sum(\"Unique Drivers\").alias(\"TotalDrivers\")\n",
    "    ) \\\n",
    "    .withColumn(\"TotalActivity\", col(\"TotalEyeballs\") + col(\"TotalRequests\") + col(\"TotalDrivers\")) \\\n",
    "    .orderBy(\"TotalActivity\") \\\n",
    "    .show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab5be5-dde5-4abc-b83e-700713ae9ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
