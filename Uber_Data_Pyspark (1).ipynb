{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b84d8b-6aa8-4675-a024-95f4ee764ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503ada43-784d-4178-b0b0-f879798d7543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in ./pyspark_env/lib/python3.13/site-packages (4.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in ./pyspark_env/lib/python3.13/site-packages (from pyspark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f324b65-e9ae-4ba6-bb64-822352d8a00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/11 10:06:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TestSpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dfcbb20-39d9-42c0-83d5-ae93eb130ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Users/DELL/Downloads/dataset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5270d2fa-4bde-4bd0-97c8-e762a4cde693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='10-Sep-12', Time (Local)=7, Eyeballs =5, Zeroes =0, Completed Trips =2, Requests =2, Unique Drivers=9),\n",
       " Row(Date=None, Time (Local)=8, Eyeballs =6, Zeroes =0, Completed Trips =2, Requests =2, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=9, Eyeballs =8, Zeroes =3, Completed Trips =0, Requests =0, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=10, Eyeballs =9, Zeroes =2, Completed Trips =0, Requests =1, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=11, Eyeballs =11, Zeroes =1, Completed Trips =4, Requests =4, Unique Drivers=11)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9708e35d-3fa3-4b70-a1a7-ecf6b60a5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using spark api\n",
    "completed_trips_by_date = df.groupBy(\"Date\").sum(\"Completed Trips \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c08fca6-235c-4e38-9864-cda97f018d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, sum(Completed Trips ): bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_trips_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf3d6e47-6614-44a4-8209-6afd5923c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“… Date with most completed trips: None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum, desc\n",
    "\n",
    "# Group by and compute total\n",
    "completed_trips_by_date = df.groupBy(\"Date\").agg(_sum(\"Completed Trips\").alias(\"TotalTrips\"))\n",
    "\n",
    "# Find date with most completed trips\n",
    "date_with_most_completed_trips = completed_trips_by_date \\\n",
    "    .orderBy(desc(\"TotalTrips\")) \\\n",
    "    .select(\"Date\") \\\n",
    "    .first()[\"Date\"]\n",
    "\n",
    "print(\"ðŸ“… Date with most completed trips:\", date_with_most_completed_trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5abf2811-c833-4baa-a47d-dc7f2819ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     Date|TotalTrips|\n",
      "+---------+----------+\n",
      "|15-Sep-12|        23|\n",
      "|23-Sep-12|        23|\n",
      "|22-Sep-12|        19|\n",
      "|16-Sep-12|        17|\n",
      "|21-Sep-12|        10|\n",
      "|18-Sep-12|         3|\n",
      "|14-Sep-12|         3|\n",
      "|20-Sep-12|         2|\n",
      "|10-Sep-12|         2|\n",
      "|11-Sep-12|         1|\n",
      "|24-Sep-12|         1|\n",
      "|13-Sep-12|         0|\n",
      "|17-Sep-12|         0|\n",
      "|12-Sep-12|         0|\n",
      "|19-Sep-12|         0|\n",
      "+---------+----------+\n",
      "\n",
      "ðŸ“… Date with most completed trips: 15-Sep-12 ðŸš— Trips: 23\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, desc\n",
    "\n",
    "# Strip spaces from column names\n",
    "uber = uber.select([col(c).alias(c.strip()) for c in uber.columns])\n",
    "\n",
    "# Drop rows where Date is null\n",
    "uber = uber.filter(col(\"Date\").isNotNull())\n",
    "\n",
    "# Ensure 'Completed Trips' is integer\n",
    "uber = uber.withColumn(\"Completed Trips\", col(\"Completed Trips\").cast(\"int\"))\n",
    "\n",
    "# Group and find total trips\n",
    "completed_trips_by_date = uber.groupBy(\"Date\").agg(_sum(\"Completed Trips\").alias(\"TotalTrips\"))\n",
    "\n",
    "# Order by total trips\n",
    "completed_trips_by_date.orderBy(desc(\"TotalTrips\")).show()\n",
    "\n",
    "# Find date with most trips\n",
    "top_row = completed_trips_by_date.orderBy(desc(\"TotalTrips\")).first()\n",
    "if top_row:\n",
    "    print(\"ðŸ“… Date with most completed trips:\", top_row[\"Date\"], \"ðŸš— Trips:\", top_row[\"TotalTrips\"])\n",
    "else:\n",
    "    print(\"âŒ No valid data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5425de90-1b2f-4c82-97c5-19e77700399f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='10-Sep-12', Time (Local)=7, Eyeballs=5, Zeroes=0, Completed Trips=2, Requests=2, Unique Drivers=9),\n",
       " Row(Date='11-Sep-12', Time (Local)=0, Eyeballs=9, Zeroes=3, Completed Trips=1, Requests=1, Unique Drivers=3),\n",
       " Row(Date='12-Sep-12', Time (Local)=0, Eyeballs=9, Zeroes=2, Completed Trips=0, Requests=1, Unique Drivers=2),\n",
       " Row(Date='13-Sep-12', Time (Local)=0, Eyeballs=11, Zeroes=11, Completed Trips=0, Requests=2, Unique Drivers=0),\n",
       " Row(Date='14-Sep-12', Time (Local)=0, Eyeballs=10, Zeroes=1, Completed Trips=3, Requests=4, Unique Drivers=3)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber.head(5) #uber has the grouped date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e4e8d1d-527d-44de-9f96-5a58da9f8564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='10-Sep-12', Time (Local)=7, Eyeballs=5, Zeroes=0, Completed Trips=2, Requests=2, Unique Drivers=9),\n",
       " Row(Date=None, Time (Local)=8, Eyeballs=6, Zeroes=0, Completed Trips=2, Requests=2, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=9, Eyeballs=8, Zeroes=3, Completed Trips=0, Requests=0, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=10, Eyeballs=9, Zeroes=2, Completed Trips=0, Requests=1, Unique Drivers=14),\n",
       " Row(Date=None, Time (Local)=11, Eyeballs=11, Zeroes=1, Completed Trips=4, Requests=4, Unique Drivers=11)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e63935d9-9b88-41c1-98d1-35bbe24e2256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 10:32:30 ERROR Executor: Exception in task 0.0 in stage 44.0 (TID 35)\n",
      "org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '08:00' could not be parsed at index 2. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/11 10:32:30 WARN TaskSetManager: Lost task 0.0 in stage 44.0 (TID 35) (macbookair.mshome.net executor driver): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text '08:00' could not be parsed at index 2. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/06/11 10:32:30 ERROR TaskSetManager: Task 0 in stage 44.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "DateTimeException",
     "evalue": "[CANNOT_PARSE_TIMESTAMP] Text '08:00' could not be parsed at index 2. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDateTimeException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     14\u001b[39m completed_trips_by_window = df \\\n\u001b[32m     15\u001b[39m     .groupBy(window(\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m24 hours\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     16\u001b[39m     .agg(\u001b[38;5;28msum\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCompleted Trips\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mTotal Completed Trips\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     17\u001b[39m     .orderBy(\u001b[33m\"\u001b[39m\u001b[33mTotal Completed Trips\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Step 4: Get the highest number of completed trips in a 24-hour window\u001b[39;00m\n\u001b[32m     20\u001b[39m highest_completed_trips_in_24_hours = \u001b[43mcompleted_trips_by_window\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTotal Completed Trips\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mTotal Completed Trips\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸš— Highest completed trips in any 24-hour window:\u001b[39m\u001b[33m\"\u001b[39m, highest_completed_trips_in_24_hours)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:940\u001b[39m, in \u001b[36mDataFrame.first\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Optional[Row]:\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:935\u001b[39m, in \u001b[36mDataFrame.head\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    933\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhead\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Union[Optional[Row], List[Row]]:\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m935\u001b[39m         rs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    937\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.take(n)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:937\u001b[39m, in \u001b[36mDataFrame.head\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    935\u001b[39m     rs = \u001b[38;5;28mself\u001b[39m.head(\u001b[32m1\u001b[39m)\n\u001b[32m    936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:460\u001b[39m, in \u001b[36mDataFrame.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) -> List[Row]:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:443\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Row]:\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m         sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mDateTimeException\u001b[39m: [CANNOT_PARSE_TIMESTAMP] Text '08:00' could not be parsed at index 2. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, lpad, to_timestamp, sum, window, lit\n",
    "\n",
    "\n",
    "# Step 1: Combine Date and Time into a proper datetime string\n",
    "df = df.withColumn(\n",
    "    \"datetime_string\",\n",
    "    concat_ws(\" \", \"Date\", concat_ws(\":\", lpad(\"Time (Local)\", 2, \"0\"), lpad(lit(\"00\"), 2, \"0\")))\n",
    ")\n",
    "\n",
    "# Step 2: Convert to timestamp (handle format like '15-Sep-12 16:00')\n",
    "df = df.withColumn(\"datetime\", to_timestamp(\"datetime_string\", \"dd-MMM-yy HH:mm\"))\n",
    "\n",
    "# Step 3: Group by 24-hour window and sum the completed trips\n",
    "completed_trips_by_window = df \\\n",
    "    .groupBy(window(\"datetime\", \"24 hours\")) \\\n",
    "    .agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\")) \\\n",
    "    .orderBy(\"Total Completed Trips\", ascending=False)\n",
    "\n",
    "# Step 4: Get the highest number of completed trips in a 24-hour window\n",
    "highest_completed_trips_in_24_hours = completed_trips_by_window \\\n",
    "    .select(\"Total Completed Trips\") \\\n",
    "    .first()[\"Total Completed Trips\"]\n",
    "\n",
    "print(\"ðŸš— Highest completed trips in any 24-hour window:\", highest_completed_trips_in_24_hours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d97f2-f2b6-4c06-a114-2d447e716b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a47b9-4a9b-4b38-a518-ee06efe3cd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "047df01b-ea68-4789-b770-9413e553b604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-06-11 10:33:16.128\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \\\"hour(Time (Local))\\\" due to data type mismatch: The first parameter requires the \\\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\\\" type, however \\\"Time (Local)\\\" has the type \\\"INT\\\". SQLSTATE: 42K09\", \"context\": {\"file\": \"java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"line\": \"\", \"fragment\": \"hour\", \"errorClass\": \"DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o695.agg.\\n: org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \\\"hour(Time (Local))\\\" due to data type mismatch: The first parameter requires the \\\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\\\" type, however \\\"Time (Local)\\\" has the type \\\"INT\\\". SQLSTATE: 42K09;\\n'Aggregate [hour(Time (Local)#42, Some(Asia/Kolkata))], [hour(Time (Local)#42, Some(Asia/Kolkata)) AS hour#428, sum(Requests#46) AS total_requests#429L]\\n+- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#407, to_timestamp(datetime_string#407, Some(dd-MMM-yy HH:mm), TimestampType, Some(Asia/Kolkata), true) AS datetime#408]\\n   +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, concat_ws(:, lpad(cast(Time (Local)#42 as string), 2, 0), lpad(00, 2, 0))) AS datetime_string#407, datetime#387]\\n      +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#386, to_timestamp(datetime_string#386, Some(dd-MMM-yy HH), TimestampType, Some(Asia/Kolkata), true) AS datetime#387]\\n         +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, lpad(cast(Time (Local)#42 as string), 2, 0)) AS datetime_string#386]\\n            +- Project [Date#17 AS Date#41, Time (Local)#18 AS Time (Local)#42, Eyeballs #19 AS Eyeballs#43, Zeroes #20 AS Zeroes#44, Completed Trips #21 AS Completed Trips#45, Requests #22 AS Requests#46, Unique Drivers#23 AS Unique Drivers#47]\\n               +- Relation [Date#17,Time (Local)#18,Eyeballs #19,Zeroes #20,Completed Trips #21,Requests #22,Unique Drivers#23] csv\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:77)\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:70)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:420)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:84)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:54)\\n\\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:158)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:147)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:77)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:70)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:420)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 22 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/Users/DELL/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/Users/DELL/Downloads/pyspark_env/lib/python3.13/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"hour(Time (Local))\" due to data type mismatch: The first parameter requires the \"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\" type, however \"Time (Local)\" has the type \"INT\". SQLSTATE: 42K09;\n'Aggregate [hour(Time (Local)#42, Some(Asia/Kolkata))], [hour(Time (Local)#42, Some(Asia/Kolkata)) AS hour#428, sum(Requests#46) AS total_requests#429L]\n+- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#407, to_timestamp(datetime_string#407, Some(dd-MMM-yy HH:mm), TimestampType, Some(Asia/Kolkata), true) AS datetime#408]\n   +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, concat_ws(:, lpad(cast(Time (Local)#42 as string), 2, 0), lpad(00, 2, 0))) AS datetime_string#407, datetime#387]\n      +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#386, to_timestamp(datetime_string#386, Some(dd-MMM-yy HH), TimestampType, Some(Asia/Kolkata), true) AS datetime#387]\n         +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, lpad(cast(Time (Local)#42 as string), 2, 0)) AS datetime_string#386]\n            +- Project [Date#17 AS Date#41, Time (Local)#18 AS Time (Local)#42, Eyeballs #19 AS Eyeballs#43, Zeroes #20 AS Zeroes#44, Completed Trips #21 AS Completed Trips#45, Requests #22 AS Requests#46, Unique Drivers#23 AS Unique Drivers#47]\n               +- Relation [Date#17,Time (Local)#18,Eyeballs #19,Zeroes #20,Completed Trips #21,Requests #22,Unique Drivers#23] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hour, \u001b[38;5;28msum\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m hourly_requests = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhour\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTime (Local)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhour\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRequests\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal_requests\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.orderBy(\u001b[33m\"\u001b[39m\u001b[33mtotal_requests\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      5\u001b[39m most_requested_hour = hourly_requests.select(\u001b[33m\"\u001b[39m\u001b[33mhour\u001b[39m\u001b[33m\"\u001b[39m).first()[\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThe hour with the most requests is:\u001b[39m\u001b[33m\"\u001b[39m, most_requested_hour)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/sql/group.py:190\u001b[39m, in \u001b[36mGroupedData.agg\u001b[39m\u001b[34m(self, *exprs)\u001b[39m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[33m\"\u001b[39m\u001b[33mall exprs should be Column\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m     exprs = cast(Tuple[Column, ...], exprs)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jgd\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.session)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/pyspark_env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"hour(Time (Local))\" due to data type mismatch: The first parameter requires the \"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\" type, however \"Time (Local)\" has the type \"INT\". SQLSTATE: 42K09;\n'Aggregate [hour(Time (Local)#42, Some(Asia/Kolkata))], [hour(Time (Local)#42, Some(Asia/Kolkata)) AS hour#428, sum(Requests#46) AS total_requests#429L]\n+- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#407, to_timestamp(datetime_string#407, Some(dd-MMM-yy HH:mm), TimestampType, Some(Asia/Kolkata), true) AS datetime#408]\n   +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, concat_ws(:, lpad(cast(Time (Local)#42 as string), 2, 0), lpad(00, 2, 0))) AS datetime_string#407, datetime#387]\n      +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, datetime_string#386, to_timestamp(datetime_string#386, Some(dd-MMM-yy HH), TimestampType, Some(Asia/Kolkata), true) AS datetime#387]\n         +- Project [Date#41, Time (Local)#42, Eyeballs#43, Zeroes#44, Completed Trips#45, Requests#46, Unique Drivers#47, concat_ws( , Date#41, lpad(cast(Time (Local)#42 as string), 2, 0)) AS datetime_string#386]\n            +- Project [Date#17 AS Date#41, Time (Local)#18 AS Time (Local)#42, Eyeballs #19 AS Eyeballs#43, Zeroes #20 AS Zeroes#44, Completed Trips #21 AS Completed Trips#45, Requests #22 AS Requests#46, Unique Drivers#23 AS Unique Drivers#47]\n               +- Relation [Date#17,Time (Local)#18,Eyeballs #19,Zeroes #20,Completed Trips #21,Requests #22,Unique Drivers#23] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, sum\n",
    "\n",
    "hourly_requests = df.groupBy(hour(\"Time (Local)\").alias(\"hour\")).agg(sum(\"Requests\").alias(\"total_requests\")).orderBy(\"total_requests\", ascending=False)\n",
    "\n",
    "most_requested_hour = hourly_requests.select(\"hour\").first()[0]\n",
    "print(\"The hour with the most requests is:\", most_requested_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dda7ecb5-4459-4b2e-86ed-215714b0a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|Time (Local)|TotalRequests|\n",
      "+------------+-------------+\n",
      "|          23|          184|\n",
      "+------------+-------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#3. Which hour of the day had the most requests during the two-week period?\n",
    "df.groupBy(\"Time (Local)\") \\\n",
    "    .agg(sum(\"Requests\").alias(\"TotalRequests\")) \\\n",
    "    .orderBy(desc(\"TotalRequests\")) \\\n",
    "    .show(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "153d7a03-bbe6-4f0f-958d-ffdcaaa6abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Weekend Zeroes: 2.31%\n"
     ]
    }
   ],
   "source": [
    "#4What percentage of all zeroes occurred on the weekend (Fri 5pm to Sun 3am)?\n",
    "from pyspark.sql.functions import hour, to_timestamp, date_format\n",
    "\n",
    "df = df.withColumn(\"Timestamp\", to_timestamp(\"Date\", \"dd-MMM-yy\")) \\\n",
    "       .withColumn(\"Weekday\", date_format(\"Timestamp\", \"E\")) \\\n",
    "       .withColumn(\"Hour\", df[\"Time (Local)\"].cast(\"int\"))\n",
    "\n",
    "# Weekend filter: Fri 17:00 - Sun 03:00\n",
    "weekend_df = df.filter(\n",
    "    ((df[\"Weekday\"] == \"Fri\") & (df[\"Hour\"] >= 17)) |\n",
    "    (df[\"Weekday\"] == \"Sat\") |\n",
    "    ((df[\"Weekday\"] == \"Sun\") & (df[\"Hour\"] < 3))\n",
    ")\n",
    "\n",
    "# Sum Zeroes\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "total_zeroes = df.agg(sum(\"Zeroes\")).collect()[0][0]\n",
    "weekend_zeroes = weekend_df.agg(sum(\"Zeroes\")).collect()[0][0]\n",
    "\n",
    "percentage = (weekend_zeroes / total_zeroes) * 100\n",
    "print(f\"ðŸ“Š Weekend Zeroes: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18a311aa-7d11-4306-ab41-7e8468515d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš– Weighted average: 0.83 trips per driver\n"
     ]
    }
   ],
   "source": [
    "#Q5.What is the weighted average ratio of completed trips per driver during the two-week period? Tip: â€œWeighted averageâ€ means your answer should account for the total trip volume in each hour to determine the most accurate number in the whole period.\n",
    "from pyspark.sql.functions import col, expr, sum\n",
    "\n",
    "# Safe division: try_divide avoids division-by-zero error\n",
    "df = df.withColumn(\"TripDriverRatio\", expr(\"try_divide(`Completed Trips`, `Unique Drivers`)\"))\n",
    "\n",
    "# Weighted average = sum(ratio * completed trips) / sum(completed trips)\n",
    "df = df.withColumn(\"Weight\", col(\"TripDriverRatio\") * col(\"Completed Trips\"))\n",
    "\n",
    "weighted_avg = df.agg((sum(\"Weight\") / sum(\"Completed Trips\")).alias(\"WeightedAvg\")) \\\n",
    "                 .first()[\"WeightedAvg\"]\n",
    "\n",
    "print(f\"âš– Weighted average: {weighted_avg:.2f} trips per driver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcfbb8ee-84b9-4416-af2d-6d15c18c2a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|  Shift|TotalRequests|\n",
      "+-------+-------------+\n",
      "|Shift_3|         1032|\n",
      "|Shift_1|          446|\n",
      "|Shift_2|          380|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q6.In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two-week period in terms of unique requests? A new shift starts every 8 hours. Assume that a driver will work the same shift each day.\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\"Shift\", when(col(\"Time (Local)\") < 8, \"Shift_1\")\n",
    "                              .when(col(\"Time (Local)\") < 16, \"Shift_2\")\n",
    "                              .otherwise(\"Shift_3\"))\n",
    "\n",
    "df_shift = df.groupBy(\"Shift\").agg(sum(\"Requests\").alias(\"TotalRequests\")) \\\n",
    "             .orderBy(desc(\"TotalRequests\"))\n",
    "\n",
    "df_shift.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59c795f7-dfd2-4721-8699-bd372ba2a753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Correlation: 0.69\n",
      "âœ… True\n"
     ]
    }
   ],
   "source": [
    "#Q7.True or False: Driver supply always increases when demand increases during the two-week period. Tip: Visualize the data to confirm your answer if needed.\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "correlation = df.select(corr(\"Requests\", \"Unique Drivers\")).first()[0]\n",
    "print(f\"ðŸ“ˆ Correlation: {correlation:.2f}\")\n",
    "print(\"âœ… True\" if correlation > 0.5 else \"âŒ False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92da7dd2-9daf-432e-8910-d26d9acb8f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 11:07:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/11 11:07:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/11 11:07:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/11 11:07:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/11 11:07:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|     Date|            Ratio|\n",
      "+---------+-----------------+\n",
      "|23-Sep-12|0.391304347826087|\n",
      "+---------+-----------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#Q8.In which 72-hour period is the ratio of Zeroes to Eyeballs the highest?\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Window over 3 days\n",
    "df = df.withColumn(\"EyeballsInt\", col(\"Eyeballs\").cast(\"int\")) \\\n",
    "       .withColumn(\"ZeroesInt\", col(\"Zeroes\").cast(\"int\"))\n",
    "\n",
    "windowSpec = Window.orderBy(\"Date\").rowsBetween(0, 71)\n",
    "\n",
    "df = df.withColumn(\"SumZeroes\", sum(\"ZeroesInt\").over(windowSpec)) \\\n",
    "       .withColumn(\"SumEyeballs\", sum(\"EyeballsInt\").over(windowSpec)) \\\n",
    "       .withColumn(\"Ratio\", col(\"SumZeroes\") / col(\"SumEyeballs\")) \\\n",
    "       .orderBy(desc(\"Ratio\"))\n",
    "\n",
    "df.select(\"Date\", \"Ratio\").show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78d986e0-fba4-4a9e-9b79-96e9cb0a7a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+------------+-----------------+\n",
      "|Time (Local)|TotalEyeballs|TotalDrivers| EyeballPerDriver|\n",
      "+------------+-------------+------------+-----------------+\n",
      "|          23|          488|         119|4.100840336134453|\n",
      "+------------+-------------+------------+-----------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#9. If you could add 5 drivers to a single hour daily, which one?\n",
    "df.groupBy(\"Time (Local)\") \\\n",
    "    .agg(\n",
    "        sum(\"Eyeballs\").alias(\"TotalEyeballs\"),\n",
    "        sum(\"Unique Drivers\").alias(\"TotalDrivers\")\n",
    "    ) \\\n",
    "    .withColumn(\"EyeballPerDriver\", col(\"TotalEyeballs\") / col(\"TotalDrivers\")) \\\n",
    "    .orderBy(desc(\"EyeballPerDriver\")) \\\n",
    "    .show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a20bd721-d94e-4533-a8b0-5307fdce52b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â° Best hour to add drivers: 23h\n",
      "ðŸ§® Pressure score: 4.10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col, desc\n",
    "\n",
    "# Group by hour of day (0-23) and calculate total eyeballs and total drivers\n",
    "hourly_demand = df.groupBy(\"Time (Local)\").agg(\n",
    "    sum(\"Eyeballs\").alias(\"TotalEyeballs\"),\n",
    "    sum(\"Unique Drivers\").alias(\"TotalDrivers\")\n",
    ")\n",
    "\n",
    "# Calculate pressure score = Eyeballs per driver, safely avoiding division by zero\n",
    "hourly_demand = hourly_demand.withColumn(\n",
    "    \"PressureScore\",\n",
    "    expr(\"try_divide(TotalEyeballs, TotalDrivers)\")\n",
    ")\n",
    "\n",
    "# Find the hour with the highest pressure score\n",
    "busiest_hour = hourly_demand.orderBy(desc(\"PressureScore\")).first()\n",
    "\n",
    "print(f\"â° Best hour to add drivers: {busiest_hour['Time (Local)']}h\")\n",
    "print(f\"ðŸ§® Pressure score: {busiest_hour['PressureScore']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec0a40f5-1ed0-4d3b-a158-06ea24455464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------+------------+-------------+\n",
      "|Time (Local)|TotalEyeballs|TotalRequests|TotalDrivers|TotalActivity|\n",
      "+------------+-------------+-------------+------------+-------------+\n",
      "|           4|           29|            9|           9|           47|\n",
      "+------------+-------------+-------------+------------+-------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#Q10.\n",
    "df.groupBy(\"Time (Local)\") \\\n",
    "    .agg(\n",
    "        sum(\"Eyeballs\").alias(\"TotalEyeballs\"),\n",
    "        sum(\"Requests\").alias(\"TotalRequests\"),\n",
    "        sum(\"Unique Drivers\").alias(\"TotalDrivers\")\n",
    "    ) \\\n",
    "    .withColumn(\"TotalActivity\", col(\"TotalEyeballs\") + col(\"TotalRequests\") + col(\"TotalDrivers\")) \\\n",
    "    .orderBy(\"TotalActivity\") \\\n",
    "    .show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab5be5-dde5-4abc-b83e-700713ae9ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
